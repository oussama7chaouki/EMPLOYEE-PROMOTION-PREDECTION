# Employee Promotion Prediction

## Project Overview

In today’s professional world, employee promotion is crucial for recognizing skills and motivating growth. Traditional evaluation methods can be influenced by subjective judgments. This project explores how machine learning can predict employee promotions using objective and measurable data, improving decision accuracy and fairness.

### Key Objectives:
- Identify high-potential employees
- Reduce subjectivity in promotion decisions
- Optimize employee development
- Ensure equitable talent management

## Introduction

This project demonstrates that predicting employee promotions based on both personal and job-specific characteristics can yield highly accurate results, assisting organizations in making informed HR decisions. Using a dataset containing attributes like education, department, and average training scores, we built and optimized classification models to predict promotion eligibility. 

AutoML tools like TPOT were integrated to automate the process of model selection and hyperparameter tuning, significantly improving prediction accuracy. Additionally, techniques like SMOTE, SMOTE + ENN, and SMOTE + Tomek were applied to address class imbalance, ensuring fairer evaluations. 

In the future, predictive models could streamline promotion processes, helping organizations to identify high-potential employees more efficiently.

---

## File Structure


- **Apply_Tpot_pipeline**: Includes the script that applies the pipeline generated by TPOT.
- **EDA**: Contains scripts and notebooks that explore and visualize the data.
- **employee_promotion**: Holds the primary dataset and related resources.
- **Generating_Tpot_pipeline**: Includes the script that generates the AutoML pipeline using TPOT.
- **Model_Building**: Contains code for traditional and automated model building, including handling class imbalance.

---

## Methodology

### Data Collection
- Dataset: **Employee Evaluation for Promotion** (Kaggle). This dataset includes employee information and their promotion status, serving as the foundation for predictive modeling.

### Exploratory Data Analysis (EDA)
- Key patterns and feature distributions were explored to understand their impact on promotion eligibility, providing insights into relationships between variables.

### Data Preprocessing
- **Imputation of Missing Values**: 
  - For `previous_year_rating`, missing values were set to 0, assuming new employees.
  - For `avg_training_score`, missing values were imputed with the mode.
  
- **Encoding Categorical Features**: 
  - `LabelEncoder` was used for the `education` feature.
  - `OneHotEncoder` was used for all other categorical variables (e.g., region, department).

### Handling Imbalanced Data
Various techniques were employed to address class imbalance:
- **SMOTE**: Generated synthetic samples for the minority class.
- **SMOTE + Tomek Links**: Combined oversampling with the removal of overlapping instances.
- **SMOTE + ENN**: Oversampled the minority class and removed noisy or borderline majority samples.
- **Undersampling**: Reduced the majority class size to balance the dataset.

### Model Building
A variety of models were evaluated, including:
- **Bagging**: Random Forest, Extra Trees Classifier
- **Boosting**: XGBoost, AdaBoost, LightGBM
- **Probabilistic Models**: Naive Bayes
- **Linear Models**: Logistic Regression
- **Neural Networks**: Artificial Neural Networks (ANN)
- **Automated ML**: TPOT for automated model selection and hyperparameter tuning.

### Model Evaluation
- **Metrics**: Accuracy, F1-score, Precision, Recall.
- **Primary Metric**: Accuracy was chosen due to its simplicity and clarity in communicating results to both technical and non-technical stakeholders.

---

## Results

| Model                     | Unbalanced Data | SMOTE | SMOTE + Tomek | SMOTE + ENN | Undersampling |
|----------------------------|-----------------|-------|---------------|-------------|---------------|
| Random Forest               | 0.93            | 0.95  | 0.95          | 0.96        | 0.70          |
| Extra Trees Classifier       | 0.93            | 0.95  | 0.95          | 0.96        | 0.69          |
| XGBoost                     | 0.94            | 0.93  | 0.92          | 0.93        | 0.70          |
| AdaBoost                    | 0.92            | 0.82  | 0.82          | 0.82        | 0.69          |
| LightGBM                    | 0.94            | 0.93  | 0.93          | 0.92        | 0.72          |
| Naive Bayes                 | 0.91            | 0.67  | 0.67          | 0.70        | 0.61          |
| Decision Tree               | 0.88            | 0.91  | 0.92          | 0.93        | 0.64          |
| K-Nearest Neighbors (KNN)   | 0.92            | 0.87  | 0.87          | 0.94        | 0.63          |
| SVM                         | 0.91            | 0.69  | 0.69          | 0.72        | 0.63          |
| Logistic Regression         | 0.92            | 0.69  | 0.68          | 0.71        | 0.67          |
| Artificial Neural Network   | 0.93            | 0.80  | 0.80          | 0.83        | 0.68          |
| **TPOT**                    | **0.94**        | 0.95  | 0.95          | **0.97**    | **0.72**      |

- **TPOT** delivered the highest accuracy, optimizing model pipelines.
- **SMOTE + ENN** provided the best class balancing.
- **Random Forest** and **Extra Trees Classifier** were strong performers, but TPOT outperformed all manual attempts.

---

## References

1. Olson, R. S., & Moore, J. H. (2016). TPOT: A tree-based pipeline optimization tool for automating machine learning. Proceedings of the Workshop on Automatic Machine Learning. [Link](https://proceedings.mlr.press/v64/olson_tpot_2016.html)
2. S. bin Shafie, S. P. Ooi, et K. W. Khaw, « Prediction of Employee Promotion Using Hybrid Sampling Method with Machine Learning Architecture », Malays. J. Comput., vol. 8, no 1, p. 1264-1286, 2023.
3. Y. Long, J. Liu, M. Fang, T. Wang, et W. Jiang, « Prediction of Employee Promotion Based on Personal Basic Features and Post Features », in Proceedings of the International Conference on Data Processing and Applications, Guangdong China: ACM, 2018. doi: [10.1145/3224207.3224210](https://doi.org/10.1145/3224207.3224210)
4. K. Sahinbas, « Employee Promotion Prediction by using Machine Learning Algorithms for Imbalanced Dataset », in 2022 2nd International Conference on Computing and Machine Intelligence (ICMI), 2022. doi: [10.1109/ICMI55296.2022.9873744](https://doi.org/10.1109/ICMI55296.2022.9873744)

---



This project was completed as part of my academic studies and aims to improve the decision-making process in HR departments by leveraging machine learning techniques.
